---
title: "709 HW4"
output:
  pdf_document:
    latex_engine: xelatex
date: "2026-02-19"
---

## 1

![](images/clipboard-1931848338.png)

### 1.a

$$\log\left(\frac{\pi_1}{\pi_3}\right) = 1.785 + 1.044 x_1 + 0.703 x_2$$

$$\log\left(\frac{\pi_2}{\pi_3}\right) = 1.554 + 0.254 x_1 - 0.106 x_2$$

$$
\begin{aligned}
\log\left(\frac{\pi_1}{\pi_2}\right) &= \log\left(\frac{\pi_1}{\pi_3}\right) - \log\left(\frac{\pi_2}{\pi_3}\right) \\
&= (1.785 + 1.044 x_1 + 0.703 x_2) - (1.554 + 0.254 x_1 - 0.106 x_2) \\
&= 0.231 + 0.790 x_1 + 0.809 x_2
\end{aligned}
$$

### 1.b

$$
\log\left(\frac{\pi_{\text{Yes}}}{\pi_{\text{No}}}\right)
=
\alpha_1+\beta_1^G x_1+\beta_1^R x_2
$$ From the table, we know $\hat\beta_1^G = 1.044, \ SE(\hat\beta_1^G) = 0.259$

```{r}
beta <- 1.044
se   <- 0.259

ci_beta <- beta + c(-1, 1) * 1.96 * se
OR      <- exp(beta)
ci_OR   <- exp(ci_beta)


cat("95% CI for beta:", ci_beta[1], "to", ci_beta[2], "\n\n")
cat("Odds Ratio:", OR, "\n")
cat("95% CI for OR:", ci_OR[1], "to", ci_OR[2], "\n")
```

Controlling for race, the estimated odds that a female believes in heaven (Yes vs No) are 2.84 times the odds for a male. The 95% confidence interval for this odds ratio is (1.71, 4.72). Since the interval does not include 1, there is significant evidence that females are more likely than males to respond “Yes” rather than “No,” given race.

### 1.c

```{r}
eta1 <- 1.785 + 1.044*1 + 0.703*0   # Yes / No
eta2 <- 1.554 + 0.254*1 - 0.106*0  # Unsure / No
pi1  <- exp(eta1) / (1 + exp(eta1) + exp(eta2))

cat("P_hat (Y = Yes)   =", round(pi1, 4), "\n")
```

### 1.d

$$
\eta_1=\log\left(\frac{\pi_1}{\pi_3}\right)=\alpha_1+\beta_1^G x_1+\beta_1^R x_2\\
\eta_2=\log\left(\frac{\pi_2}{\pi_3}\right)=\alpha_2+\beta_2^G x_1+\beta_2^R x_2
$$

-   For white males, $x_1=0，x_2=0$: $\eta_1=\alpha_1=1.785,\quad \eta_2=\alpha_2=1.554$

This means:

$$\eta_1 > 0 \implies \pi_1/\pi_3 = e^{\eta_1} > 1 \implies \pi_1 > \pi_3$$ $$\eta_2 > 0 \implies \pi_2/\pi_3 = e^{\eta_2} > 1 \implies \pi_2 > \pi_3$$

Since

$$\log\left(\frac{\pi_1}{\pi_2}\right) = \eta_1 - \eta_2 = 0.231 > 0 \implies \pi_1 > \pi_2$$

We have $\pi_1>\pi_2>\pi_3$

-   For black females $x_1=1，x_2=1$:

$$
\eta_1^{BF}=1.785 + 1.044(1) + 0.703(1)=3.532
$$

$$
\eta_2^{BF}=1.554 + 0.254(1) - 0.106(1)=1.702
$$

Similarly,

$$
\begin{aligned}
\eta_1^{BF} > 0 &\implies \pi_1 > \pi_3 \\
\eta_2^{BF} > 0 &\implies \pi_2 > \pi_3 \\
\eta_1^{BF} - \eta_2^{BF} = 3.532 - 1.702 = 1.830 > 0 &\implies \pi_1 > \pi_2
\end{aligned}
$$

Therefore, $\pi_1>\pi_2>\pi_3$ for black females.

### 1.f

-   df = 2

The data form a $2 \times 2 \times 3$ table (gender × race × belief category). Thus there are 4 gender–race combinations, and within each combination there are 3 response probabilities subject to the constraint that they sum to 1. Thus each combination contributes 3 - 1 = 2 free parameters.

The saturated model therefore has: $4 \times 2 = 8$ free parameters.

The fitted baseline-category logit model contains, for each of the two logits (Yes/No and Unsure/No): 1 intercept, 1 gender effect and 1 race effect

This gives: $2 \times 3 = 6$ parameters.

Hence the residual degrees of freedom are: $8 - 6 = 2$.

```{r}
G2_full <- 0.69
G2_red  <- 47.64

LR  <- G2_red - G2_full
df  <- 2
pval <- pchisq(LR, df = df, lower.tail = FALSE)

cat("LR test for deleting gender (given race):\n")
cat("  LR statistic =", LR, "with df =", df, "\n")
cat("  p-value      =", format.pval(pval, digits = 4), "\n")
```

Given the p-value is less than 0.001, we strongly reject the null hypothesis of independence. Therefore, there is very strong evidence that, even after controlling for race, gender is significantly associated with belief in the existence of heaven.

## 2

![](images/clipboard-148856408.png)

![](images/clipboard-1538116160.png)

### 2.a

There are four intercepts because the response variable has five ordered categories. In the proportional odds model, we model four cumulative logits:

$$
\log\frac{P(Y \le j)}{P(Y > j)}, \quad j=1,2,3,4.
$$

For males in urban areas wearing seat belts (the reference group), all covariates equal zero. Therefore, the cumulative logits equal the intercepts. These intercepts determine the cumulative probabilities for this baseline group, and from them we obtain the full estimated response distribution

### 2.b

```{r}
beta <- -0.5463
se   <-  0.0272

ci_beta <- beta + c(-1, 1) * 1.96 * se
OR      <- exp(beta)
ci_OR   <- exp(ci_beta)

cat("Gender effect (female vs male):\n")
cat("  beta_hat =", beta, "\n")
cat("  95% CI for beta:", round(ci_beta[1],4), "to", round(ci_beta[2],4), "\n\n")

cat("Cumulative odds ratio (female vs male):\n")
cat("  OR_hat =", round(OR,4), "\n")
cat("  95% CI for OR:", round(ci_OR[1],4), "to", round(ci_OR[2],4), "\n")
```

Holding seat-belt use and location fixed, the estimated cumulative odds ratio for females versus males is 0.5791 (95% CI: 0.549 to 0.6108 ).

Thus, compared with males, females are more likely to have more severe injury outcomes than males, after adjusting for seat-belt use and location.

### 2.c

proportional odds model:

$$
\log\frac{P(Y\le j)}{P(Y>j)}=\alpha_j+\beta_G(\text{female})+\beta_L(\text{rural})+\beta_S(\text{no seatbelt})+\beta_{LS}(\text{rural}\times \text{no})
$$

where $\beta_S = -0.7602,  \quad \beta_{LS} = -0.1244.$

```{r}
beta_seat_no <- -0.7602
beta_int     <- -0.1244   # rural * no seatbelt

# Urban: rural = 0
logOR_urban <- beta_seat_no
OR_urban    <- exp(logOR_urban)

# Rural: rural = 1
logOR_rural <- beta_seat_no + beta_int
OR_rural    <- exp(logOR_rural)


ratio_effect <- OR_rural / OR_urban

cat("Cumulative Odds Ratio (No vs Yes):\n")
cat("  Urban :", round(OR_urban, 4), "\n")
cat("  Rural :", round(OR_rural, 4), "\n\n")

cat("Ratio (Rural / Urban):", round(ratio_effect, 4),
    "  (equals exp(-0.1244))\n")
```

The estimated cumulative odds ratio is $0.4676$, which means in urban areas and holding gender fixed, individuals not wearing seat belts have about 0.47 times the cumulative odds of being in category \$Y \<= j (versus Y \> j) compared with those wearing seat belts, for every cutpoint j=1,2,3,4.

While in rural areas, individuals not wearing seat belts have about 0.4129 times the cumulative odds of being in lower injury categories compared with those wearing seat belts.

The cumulative odds ratio is smaller in rural areas than in urban areas, indicating that the adverse effect of not wearing a seat belt is stronger in rural areas.

The interaction estimate −0.1244 shows that the log cumulative odds for “no seat belt” are further reduced in rural areas. So the seat-belt effect in rural areas is about $\exp(-0.1244) \approx 0.88$ times that in urban areas, indicating harmful effect of not wearing a seat belt is stronger in rural areas.

## 3

![](images/clipboard-2986132409.png)

![![](images/clipboard-1836051657.png)](images/clipboard-4236093568.png)

```{r}
dat <- data.frame(
  lake = rep(c("Hancock","Oklawaha","Trafford","George"), each = 4),
  gender = rep(c("Male","Male","Female","Female"), times = 4),
  size = rep(c("Small","Large","Small","Large"), times = 4),
  Fish = c(
    7,4,16,3,      # Hancock
    2,13,3,0,      # Oklawaha
    3,8,2,0,       # Trafford
    13,9,3,8       # George
  ),
  Invertebrate = c(
    1,0,3,0,       # Hancock
    2,7,9,1,       # Oklawaha
    7,6,4,1,       # Trafford
    10,0,9,1       # George
  )
)


dat$lake <- relevel(factor(dat$lake), ref = "George")
dat$size <- relevel(factor(dat$size), ref = "Large")

fit_IF <- glm(cbind(Invertebrate, Fish) ~ size + lake,
              family = binomial,
              data = dat)

summary(fit_IF)
coefs <- summary(fit_IF)$coef

cat("Binary logistic fit for log(pi_I / pi_F):\n")
cat("Intercept =", round(coefs[1,1],2), "\n")
cat("Size =", round(coefs["sizeSmall",1],2),
    " SE =", round(coefs["sizeSmall",2],2), "\n")
cat("Hancock =", round(coefs["lakeHancock",1],2),
    " SE =", round(coefs["lakeHancock",2],2), "\n")
cat("Oklawaha =", round(coefs["lakeOklawaha",1],2),
    " SE =", round(coefs["lakeOklawaha",2],2), "\n")
cat("Trafford =", round(coefs["lakeTrafford",1],2),
    " SE =", round(coefs["lakeTrafford",2],2), "\n")
```

Thus, it is $\log(\hat\pi_I/\hat\pi_F) = -1.69 + 1.66 s - 1.78 z_H + 1.05 z_O + 1.22 z_T$ with SE $(0.43,\; 0.62,\; 0.49,\; 0.52)$.

Comparing with the first row in Table 8.4, the estimates are very similar. The standard errors are slightly larger, reflecting some loss of efficiency from fitting the binary model separately.

## 4

### 4.a

$$
\text{logit}\{P(Y \le j \mid X = x_i)\} = \alpha_j + \beta x_i,
$$ Since $x_i = i, x_{i+1} = i + 1$ $$
\begin{aligned}
\text{logit}\{P(Y \le j \mid X = x_{i+1})\}-\text{logit}\{P(Y \le j \mid X = x_i)\}
&=(\alpha_j + \beta x_{i+1})-(\alpha_j + \beta x_i) \\
&=\beta (x_{i+1} - x_i)\\
&=\beta
\end{aligned}
$$

the cumulative odds for row i is $\frac{P(Y \le j \mid X = x_i)}{P(Y > j \mid X = x_i)},$ and for row i+1, $\frac{P(Y \le j \mid X = x_{i+1})}{P(Y > j \mid X = x_{i+1})}.$

Thus, the cumulative OR is $\frac{\frac{P(Y \le j \mid X = x_{i+1})}{P(Y > j \mid X = x_{i+1})}}{\frac{P(Y \le j \mid X = x_i)}{P(Y > j \mid X = x_i)}}.$

The log cumulative OR is:

$$
\begin{aligned}
\log(\frac{\frac{P(Y \le j \mid X = x_{i+1})}{P(Y > j \mid X = x_{i+1})}}{\frac{P(Y \le j \mid X = x_i)}{P(Y > j \mid X = x_i)}}) 
&= \log\left(\frac{P(Y \le j \mid X = x_{i+1})}{P(Y > j \mid X = x_{i+1})}\right) - \log\left(\frac{P(Y \le j \mid X = x_i)}{P(Y > j \mid X = x_i)}\right)\\
&= \text{logit}\{P(Y \le j \mid X = x_{i+1})\} - \text{logit}\{P(Y \le j \mid X = x_i)\}\\
&= \beta
\end{aligned}
$$

Therefore, the cumulative OR is $e^{\beta}$, which does not depend on the i and j. Thus it is a uniform association model in cumulative odds ratios.

### 4.b

(i) In the conditional modeling framework, we consider the distribution $Y \mid X$. For each of the I rows, there are J category probabilities that sum to 1, so each row contributes J-1 free parameters. Hence, the saturated model for $Y \mid X$ contains I(J-1) parameters. Model (8.23) is $\text{logit}\{P(Y \le j \mid X=x_i)\} = \alpha_j + \beta x_i$, which includes J-1 intercept parameters and one slope parameter, for a total of J parameters. Therefore, the residual degrees of freedom are $I(J-1) - J = IJ - I - J$.
(ii) If $\beta = 0$, $P(Y\le j \mid X=x_i) = expit(\alpha_j)$ do not depend on $x_i$, implying that the conditional distribution $P(Y=j \mid X=x_i)$ is the same for all i, i.e., $P(Y=j\mid X=x_i)=P(Y=j) \quad \text{for all } i \text{ and } j,$. Thus, X and Y are independent.

## 5

![](images/clipboard-2849351855.png)

### 5.a

Since

$$
F_Z(z)=P(Z\le z)=\frac{1}{1+e^{-(z-\mu)}},\quad \mu=\gamma_0+\gamma_1 x.
$$ So $$
P(Z\le \alpha_j)=F_Z(\alpha_j)=\frac{1}{1+e^{-(\alpha_j-\mu)}}
=\frac{1}{1+e^{-(\alpha_j-(\gamma_0+\gamma_1 x))}}.
$$

### 5.b

$$
\begin{aligned}
\log\frac{P(Z\le \alpha_j)}{P(Z>\alpha_j)}
&=\log\frac{P(Z\le \alpha_j)}{1-P(Z\le \alpha_j)}\\
&= \log (e^{(\alpha_j-\mu)})\\
&=\alpha_j-\mu\\
&=\alpha_j-(\gamma_0+\gamma_1 x)\\
\end{aligned}
$$

### 5.c

$$
Y=
\begin{cases}
1\ (\text{never}), & Z\le \alpha_1,\\
2\ (\text{sometimes}), & \alpha_1<Z\le \alpha_2,\\
3\ (\text{always}), & Z>\alpha_2.
\end{cases}
$$

Thus, $Y \le (\text{sometimes})$ means $Z \le \alpha_2.$

Cumulative log odds:

$$
\begin{aligned}
\log\frac{P(Y\le \text{sometimes})}{P(Y>\text{sometimes})}
&=\log\frac{P(Z\le \alpha_2)}{P(Z>\alpha_2)}\\
&=\log\frac{P(Z\le \alpha_2)}{1 - P(Z \le \alpha_2)}\\
&=\alpha_2-(\gamma_0+\gamma_1 x) \quad \text{by (a)}
\end{aligned}
$$

### 5.d

By (c), we know $\log\frac{P(Y\le j)}{P(Y> j)} = \alpha_j-(\gamma_0+\gamma_1 x) = (\alpha_j-\gamma_0) - \gamma_1 x$

Since the proportional odds model is $\log\frac{P(Y\le j)}{P(Y>j)}=\beta_{0j}+\beta_1 x$.

We have $\beta_{0j} = \alpha_j-\gamma_0, \quad \beta_1 = -\gamma_1.$

Given Values: $\gamma_0 = 2$, $\gamma_1 = 0.5$, $\alpha_1 = -2$, $\alpha_2 = 2$.

Thus, we have

$$\beta_{01} = \alpha_1 - \gamma_0 = -2 - 2 = -4$$

$$\beta_{02} = \alpha_2 - \gamma_0 = 2 - 2 = 0$$

$$\beta_1 = -\gamma_1 = -0.5$$

### 5.e

Given the values $x = -2$, $\gamma_0 = 2$, and $\gamma_1 = 0.5$, $\alpha_1 = -2$, $\alpha_2 = 2$, we have:

$$\mu = \gamma_0 + \gamma_1 x = 2 + 0.5(-2) = 1$$ $$
Y=
\begin{cases}
1\ (\text{never}), & Z\le -2,\\
2\ (\text{sometimes}), & -2 <Z\le 2,\\
3\ (\text{always}), & Z>2.
\end{cases}
$$

$$
P(Y=\text{never})=P(Z\le -2)=F(-2)\\
=\frac{1}{1+e^{-(-2-1)}}=\frac{1}{1+e^{3}}\approx 0.04743.
$$

$$
P(Y=\text{sometimes})= P(-2 <Z\le 2) = F(2)-F(-2) \\
= \frac{1}{1+e^{-(2-1)}} -\frac{1}{1+e^{-(-2-1)}} \approx 0.73106-0.04743= 0.68363,
$$

$$
P(Y=\text{always})=P (Z>2) = 1-F(2)\approx 1-0.73106=0.26894.
$$

### 5.f

Similarly, with given values, we have:

$$\mu = \gamma_0 + \gamma_1 x = 2 + 0.5(2) = 3$$

$$
Y=
\begin{cases}
1\ (\text{never}), & Z\le -2,\\
2\ (\text{sometimes}), & -2 <Z\le 2,\\
3\ (\text{always}), & Z>2.
\end{cases}
$$

$$
P(Y=\text{never})=P(Z\le -2)=F(-2)=\frac{1}{1+e^{-(-2-3)}}\\
=\frac{1}{1+e^{5}}\approx 0.0066929
$$

$$
P(Y=\text{sometimes})= P(-2 <Z\le 2) = F(2)-F(-2) \\
= \frac{1}{1+e^{-(2-3)}} -\frac{1}{1+e^{-(-2-3)}} \approx 0.2689414-0.0066929= 0.2622485,
$$

$$
P(Y=\text{always})=P (Z>2) = 1-F(2)\approx 1-0.2689414=0.7310586
$$

### 5.g

Proportional odds model is: $\log\frac{P(Y\le j\mid x)}{P(Y>j\mid x)}=\beta_{0j}+\beta_1 x$

Thus, we have $\frac{P(Y\le j\mid x)}{P(Y>j\mid x)}=\exp(\beta_{0j}+\beta_1 x).$

Odds of being in category j+1 or higher:

$$
\begin{aligned}
\text{odds}&=\frac{P(Y\ge j+1\mid x)}{P(Y\le j\mid x)}\\
&=\frac{P(Y>j\mid x)}{P(Y\le j\mid x)} \\
&= (\frac{P(Y\le j\mid x)}{P(Y>j\mid x)})^{-1}\\
&=(\exp(\beta_{0j}+\beta_1 x))^{-1}\\
&=\exp(-\beta_{0j})\exp(-\beta_1 x)
\end{aligned}
$$

Increase x by one unit on the odds of Y, then the OR becomes :

$$
\frac{\exp(-\beta_{0j})\exp(-\beta_1 (x+1))}{\exp(-\beta_{0j})\exp(-\beta_1 (x))} = \exp(-\beta_1) = \exp(-(-0.5)) \approx 1.65
$$

In the proportional odds model, when the coefficient of x is -0.5, for a one-unit increase in x, the odds of being in a higher category (j+1 or above) versus category j or below are multiplied by 1.65.

## 6

![](images/clipboard-3768166954.png)

### 6.a

```{r message=FALSE, warning=FALSE}
set.seed(2026)
n <- 10000

gamma0 <- 2
gamma1 <- 0.5
alpha1 <- -2
alpha2 <-  2

# Assume X follows Normal
x <- rnorm(n, mean = 0, sd = 1)
mu <- gamma0 + gamma1 * x

# U ~ Unif(0,1), then Z = mu + log(U/(1-U))
u <- runif(n)
z <- mu + log(u / (1 - u))


Y <- cut(
  z,
  breaks = c(-Inf, alpha1, alpha2, Inf),
  labels = c("never", "sometimes", "always"),
  right = TRUE, include.lowest = TRUE
)

Y <- ordered(Y, levels = c("never", "sometimes", "always"))
dat <- data.frame(Y = Y, x = x, z = z, mu = mu)
head(dat)
```

### 6.b

```{r message=FALSE, warning=FALSE}
library(MASS)
fit_polr <- polr(Y ~ x, data = dat, method = "logistic", Hess = TRUE)

beta_hat  <- coef(fit_polr) 
zeta_hat  <- fit_polr$zeta 

cat("beta_hat: ", beta_hat, "\n")
cat("zeta_hat: ", zeta_hat, "\nn")


sum_polr <- summary(fit_polr)
sum_polr

# compare with 5e
true_beta0 <- c("never|sometimes" = -4, "sometimes|always" = 0) 
true_beta1 <- -0.5

comp <- rbind(
  polr_est = c(zeta_hat, beta_hat),
  truth_in_polr_param = c(true_beta0, beta_polr = true_beta1)
)
comp

```

The estimated slope from polr is 0.506, which is very close to the theoretical value = 0.5. The difference in sign compared to 5e is because polr uses the form $\log\frac{P(Y\le j)}{P(Y>j)}=\zeta_j-\beta x$, whereas the 5e used $\beta_{0j}+\beta_1 x$. Thus, $\beta_{\text{polr}} = -\beta_1$. The estimated cutpoints (-4.10 and -0.02) are also very close to their true values (-4 and 0) and the stimated cutpoints in 5e.

### 6.c

```{r message=FALSE, warning=FALSE}
library(VGAM)

fit_vglm <- vglm(
  Y ~ x,
  family = propodds(),
  data = dat
)

summary(fit_vglm)
```

Using VGAM::vglm, the estimated slope for x is 0.506 and the cutpoints are 4.10 and 0.02. In 5e, the model is $\log \frac{P(Y \le j)}{P(Y > j)} = \beta_{0j} + \beta_1 x$, with true value $\beta_1 = -0.5, \beta_{01} = -4, \beta_{02} = 0$. However, VGAM models $\log \frac{P(Y > j)}{P(Y \le j)}$, which is the reciprocal form, so the estiamtes changes sign but their abs values are closed.
